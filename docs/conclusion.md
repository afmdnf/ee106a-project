---
order: 40
---

# Conclusion

Our project successfully met our most important goals of being able to identify different types of kitchenware, pick them up, and move them to designated locations based on their classification.  We focused greatly on increasing the reliability of our algorithms before moving onto more advanced goals. Some of these advanced goals include passing the object from Baxter's right hand to its left or placing the object outside the camera field of view instead of dropping the object, both of which we were unable to implement due to time constraints, but hope to continue working on in the future. Although the video in our results section of Baxter picking up the various kitchenware appears on a relatively normal time-scale, the motion-planning steps can take ranging from a few seconds to the order of a few minutes to correctly devise a motion plan; for demonstration purposes, we cut out frames where the robot is sitting still and planning motion. This is largely a result of using Moveit as our motion planner, and implementing our own, more robust planner would be the next aspect of reliability we would choose to focus upon.   

## Difficulties We Faced
### Setup
* When calibrating the RealSense with the Baxter, a major issue was deciding where to launch the camera and our static transform publisher. We tried SSH-ing into Baxter and running all our commands, but when we did so, the speed of all our nodes greatly slowed down. Thus, we found out that if the local computer's URI was set to that of the Baxter, we could run all of our computer vision and setup nodes locally while having the information passed to Baxter. This greatly sped up the execution of our code. 
* Often, and seemingly randomly, we found that `ar_track.launch` was publishing incorrect transforms from Baxter to the AR tag, which caused our RealSense localization to fail. After several days, we determined that Baxter had recently restarted and upon startup the camera resolution was 320x200, while the AR tracker assumes that the Baxter wrist camera is at the full resolution of 1280x800. As soon as we fixed this, the transform became accurate and our static transform publisher correctly localized the RealSense.

### Object Detection and Classification
* Separating our kitchenware of interest from the background turned out to be more difficult than expected. We initially tried the several different methods taught in Lab 6 (RGB color thresholding, Canny edge detection, and k-means clustering), but we found that none of them met our requirements of isolating the objects reliably AND quickly. Furthermore, introducing more objects would cause the RealSense to automatically adjust its exposure, which would change the necessary thresholds. After looking into other options online, and thanks to Fayyaz's previous image analysis experience, we decided on using the hue-saturation-value (HSV) color space with the assumption that all of our objects are colored white, because most typical kitchenware is white. We also originally had a color threshold for brightly colored objects (ie. neon green, light red, sky blue, etc), but we found that detecting these objects was highly dependent upon the lighting condition even in the HSV color space. However, white objects under the HSV color space typically cluster in the same general region despite lighting conditions, so our color thresholding worked quite reliably on them at the several different locations in the room we tested it.
* When performing PCA, we found that the normalized principal components of cups and forks are relatively stable and consistent with our expectations. However, we often couldn't find any sort of recognizable pattern in the cup's singular values. This was because our pointcloud could only be obtained from one perspective, so much of the detected cup depended on the angle of the RealSense, how much of the inside of the cup was visible, and how much of the base of the cup was obscured by shadow. Consequently, we couldn't write an effective conditional statement to classify an object as a cup, so we made the assumption that the only type of objects in the field of view could be plates, cups, or spoons, and left the cup condition as an else statement if the plate and cup PC thresholds failed. Ideally in the future, we would like to devise a setup that allows for a constant camera angle and lighting environment so that we could develop a more robust classification algorithm without that limiting assumption.

## Planning and Actuation
* Cup: Some of the most difficult parts of programming the cup pickup sequence included detecting
* Plate: Since the most stable way to grasp the plate is similar to how a human hand grasps it, approaching from the side and grasping the top and bottom of the plate, we programmed the robot to follow that motion. Baxter would lower its forearm to a horizontal position, position the tip of the gripper over the center of the plate, move its forearm back towards its body by the radius of the plate, lower its forearm to table height, and slide its forearm forward to pick up the plate. The major difficulty came in the motion planning for getting Baxter's forearm to a horizontal position. If the plate was too close to Baxter's body, even if technically in a reachable position by Baxter, the motion planning for the plate often failed because Baxter couldn't get its arm into the position with its forearm horizontal. Sometimes even if it did, it couldn't retract its arm away from the plate by a distance equal to the radius of the plate. This is a major difficulty that we were still working on fixing in the last stages of our project while improving the reliability of the plate pickup.

## Potential Improvements
**TODO**