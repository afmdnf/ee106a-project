---
order: 40
---

# Conclusion

Our project successfully met our most important goals of being able to identify different types of kitchenware, pick them up, and move them to designated locations based on their classification.  We focused greatly on increasing the reliability of our algorithms before moving onto more advanced goals. Some of these advanced goals included passing the object from Baxter's right hand to its left or placing the object outside the camera field of view instead of dropping the object, both of which we were unable to implement due to time constraints, but hope to continue working on in the future. Although the video in our results section of Baxter picking up the various kitchenware appears on a relatively normal time-scale, the motion-planning steps can take ranging from a few seconds to the order of a few minutes to correctly devise a motion plan. Thus, for demonstration purpose, we cut out frames where the robot is sitting still and planning motion. This is largely a result of using Moveit as our motion planner, and implementing our own more robust planner would be the next aspect of reliability we would choose to focus upon.   

## Difficulties We Faced
### Setup
* When calibrating the RealSense with the Baxter, a major issue was deciding where to launch the camera and our static transform publisher. We tried SSH-ing into Baxter and running all our commands, but when we did so, the speed of all our nodes greatly slowed down. Thus, we found out that if the local computer's URI was set to that of the Baxter, we could run all of our computer vision and setup nodes locally while having the information passed to Baxter. This greatly sped up the execution of our code. 
* Often, and seemingly randomly, we found that `ar_track.launch` was publishing incorrect transforms from Baxter to the AR tag, which caused our RealSense localization to fail. After several days, we determined that Baxter had recently restarted and upon startup the camera resolution was 320x200, while the AR tracker assumes that the Baxter wrist camera is at the full resolution of 1280x800. As soon as we fixed this, the transform became accurate and our static transform publisher correctly localized the RealSense.

### Object Detection and Classification
* Separating our kitchenware of interest from the background turned out to be more difficult than expected. We initially tried the several different methods taught in Lab 6 (RGB color thresholding, Canny edge detection, and k-means clustering), but we found that none of them met our requirements of isolating the objects reliably AND quickly. Furthermore, introducing more objects would cause the RealSense to automatically adjust its exposure, which would change the necessary thresholds. After looking into other options online, and thanks to Fayyaz's previous image analysis experience, we decided on using the hue-saturation-value (HSV) color space with the assumption that all of our objects are colored white, because most typical kitchenware is white. We also originally had a color threshold for brightly colored objects (ie. neon green, light red, sky blue, etc), but we found that detecting these objects was highly dependent upon the lighting condition even in the HSV color space. However, white objects under the HSV color space typically cluster in the same general region despite lighting conditions, so our color thresholding worked quite reliably on them at the several different locations in the room we tested it.
* When performing PCA, we found that the normalized principal components of cups and forks are relatively stable and consistent with our expectations. However, we often couldn't find any sort of recognizable pattern in the cup's singular values. This was because our pointcloud could only be obtained from one perspective, so much of the detected cup depended on the angle of the RealSense, how much of the inside of the cup was visible, and how much of the base of the cup was obscured by shadow. Consequently, we couldn't write an effective conditional statement to classify an object as a cup, so we made the assumption that the only type of objects in the field of view could be plates, cups, or spoons, and left the cup condition as an else statement if the plate and cup PC thresholds failed. Ideally in the future, we would like to devise a setup that allows for a constant camera angle and lighting environment so that we could develop a more robust classification algorithm without that limiting assumption.

## Planning and Actuation
* Cup and Fork: One of the major difficulties in grasping the cup was handling how thin the rim was and making sure that our algorithm was accurate enough that errors because of the pointcloud were corrected for. When utilizing only the position information from the Realsense camera, we found that by using the median of the pointcloud, we could be telling the robot that the center of the cup was one or two centimeters away from where it actually was. Since the error needed to be even smaller than that to consistently grasp the cup, this forced us to use visual servoing to compensate. We used the same procedure for the fork to correct for similar errors because the precision needed to be about the same as the rim of the cup to pick up the plastic fork at its neck. Another smaller difficulty we faced was getting our system to work on several different tables, since we simplified our testing to a particular table height, but this would be a quick fix if we used the height of the AR-tag when doing the calibration between the Baxter and Realsense.
* Plate: Since the most stable way to grasp the plate is similar to how a human hand grasps it, approaching from the side and grasping the top and bottom of the plate, we programmed the robot to follow that motion. Baxter would lower its forearm to a horizontal position, position the tip of the gripper over the center of the plate, move its forearm back towards its body by the radius of the plate, lower its forearm to table height, and slide its forearm forward to pick up the plate. The major difficulty came in the motion planning for getting Baxter's forearm to a horizontal position. If the plate was too close to Baxter's body, even if technically in a reachable position by Baxter, the motion planning for the plate often failed because Baxter couldn't get its arm into the position with its forearm horizontal. Sometimes even if it did, it couldn't retract its arm away from the plate by a distance equal to the radius of the plate. This is a major difficulty that we were still working on fixing in the last stages of our project, to improve the reliability of the plate pickup.

## Potential Improvements

One major area of improvement which could streamline our entire process would be to implement a new motion controller, since currently we utilize the MoveIt motion planner to calculate the inverse kinematics and trajectory that Baxter will take. Moveit often fails to find a path from the tucked arm position to our calibration position even if the object is clearly within the robot’s workspace. This is a major issue that greatly slows down our organization pipeline from a process that should take roughly 30 seconds to a minute (for 3-4 objects) to one that takes about 3-5 minutes. Although writing a new controller would be very complicated, the potential gain in speed would be well worth it, and since we know the general positions that we want the arm in for each of the object types, we could likely make some simplifying assumptions when designing a controller.
