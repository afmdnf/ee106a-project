---
order: 40
---

# Conclusion

Our project successfully met our most important goals of being able to identify different types of kitchenware, pick them up, and move them to designated locations based on their classification.  We focused greatly on increasing the reliability of our algorithms before moving onto more advanced goals. Some of these advanced goals included passing the object from Baxter's right hand to its left or placing the object outside the camera field of view instead of dropping the object, both of which we would hope to continue working on in the future. Although the video in our results section of Baxter picking up the various kitchenware appears on a relatively normal time-scale, the motion-planning steps can take ranging from a few seconds to the order of a few minutes to correctly devise a motion plan. Thus, for demonstration purpose, we cut out frames where the robot is sitting still and planning motion. This is largely a result of using Moveit as our motion planner, and implementing our own more robust planner would be the next aspect of reliability we would choose to focus upon.   

## Difficulties We Faced
Difficulties in Setup:
To pass 3-dimensional object location data from the Realsense Camera to the Baxter, we had to connect the TF tree from Baxter to the Realsense via an AR marker. A major difficulty we encountered when connecting the trees was deciding where to launch the camera and our static transform publisher. We tried SSH-ing into Baxter and running all our commands, but when we did so, the speed of all our nodes greatly slowed down. Thus, we found out that if the local computer's URI was set to that of the Baxter, we could run all of our computer vision and setup nodes locally while having the information passed to Baxter. This greatly sped up the execution of our code. Another difficulty we faced in setup was seemingly randomly having incorrect transforms from Baxter to the AR tag, which caused our static transform publisher to publish an incorrect transform. We found the issue after several days of being perplexed: Baxter had recently restarted and upon startup the camera resolution was 320x200 while our code assumed that the Baxter wrist camera was at the full resolution of 1280x800. As soon as we fixed this, the transform became accurate and our static transform publisher correctly published the transform.

Difficulties in Object Detection and Classification:
The first major difficulty in object detection was figuring out an effective way to separate our kitchenware of interest from the background. We tried out several different of the methods used in Lab 6 (RGB Color Thresholding, Edge Detection, K-Means Clustering), but we found that none of them met our requirements of reliably separating the object while doing so in a computationally feasible time. After looking into other options online and confirming with Fayyaz's previous image analysis experience, we decided on using the hue-saturation-value (HSV) color space with the assumption that all of our objects are colored white, because most typical kitchenware is white. We also originally had a color threshold for brightly colored objects (ie. neon green, light red, sky blue, etc), but we found that detecting these objects was highly dependent upon the lighting condition even in the HSV color space. White objects under the HSV color space typically cluster in the same general region despite lighting conditions, so our color thresholding worked quite reliably on them at the several different locations in the room we tested it.

To classify the object in the camera frame as a plate, cup, or utensil, we utilized Principal Component Analysis (PCA) on the segmented pointcloud. To compare different sized objects, we normalized all the PCA components by the first PCA component, and when we did, we found that the plate consistently had its PC1 and PC2 both around 1 with a PC3 of about 0, and the fork/spoon had the PC2 consistently below 0.5 or so with a variable PC3. However, we couldn't find any sort of recognizable pattern in the cup's PCA of its pointcloud. This was because so much of the detected cup depended on the angle of the Realsense towards the cup, how much of the inside of the cup was visible, and how much of the base of the cup was obscured by shadow. Consequently, we couldn't write an effective conditional statement to classify an object as a cup, so we made the assumption that the only type of objects in the field of view could be plates, cups, or spoons, and left the cup condition as an else statement if the plate and cup PC thresholds failed. Ideally in the future, we would like to devise a setup that allows for a constant camera angle and lighting environment so that we could develop a more robust classification algorithm without that limiting assumption.

Difficulties in Grasping Kitchenware:


## Potential Improvements

We used AR-tags to distinguish the different objects but ideally we would want to program the robot to be able to identify different objects without AR-tags or be able to have it see and recognize smaller tags that we could attach to the objects.

We would also want to find a way to make the AR-tag tracker update and find the tf transformations faster. At the moment, our robot pauses every few seconds in order to find the tag and recalculate the transforms and see how far and at what angle the tag is after the robot has moved so that it will stay on track to get to the target.

Additionally, our robot's arm is hardcoded at the moment to always reach to the same point in front of the robot. Given more time, we would have attempted to implement a program to perform inverse kinematics on the location of the object and the arm's position and we would have also implemented controls on the arm as well so that it would reach towards the object instead of moving to the same spot. 

Finally, we would have liked to make a URDF (Universal Robot Description Format) of our robot in order to use RViz MoveIt Control.
