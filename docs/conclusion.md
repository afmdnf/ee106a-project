---
order: 40
---

# Conclusion

Our project successfully met our most important goals of being able to identify different types of kitchenware, pick them up, and move them to designated locations based on their classification.  We focused greatly on increasing the reliability of our algorithms before moving onto more advanced goals. Some of these advanced goals included passing the object from Baxter's right hand to its left or placing the object outside the camera field of view instead of dropping the object, both of which we would hope to continue working on in the future. Although the video in our results section of Baxter picking up the various kitchenware appears on a relatively normal time-scale, the motion-planning steps can take ranging from a few seconds to the order of a few minutes to correctly devise a motion plan. Thus, for demonstration purpose, we cut out frames where the robot is sitting still and planning motion. This is largely a result of using Moveit as our motion planner, and implementing our own more robust planner would be the next aspect of reliability we would choose to focus upon.   

## Difficulties We Faced
Difficulties in Setup:
To pass 3-dimensional object location data from the Realsense Camera to the Baxter, we had to connect the TF tree from Baxter to the Realsense via an AR marker. A major difficulty we encountered when connecting the trees was deciding where to launch the camera and our static transform publisher. We tried SSH-ing into Baxter and running all our commands, but when we did so, the speed of all our nodes greatly slowed down. Thus, we found out that if the local computer's URI was set to that of the Baxter, we could run all of our computer vision and setup nodes locally while having the information passed to Baxter. This greatly sped up the execution of our code. Another difficulty we faced in setup was seemingly randomly having incorrect transforms from Baxter to the AR tag, which caused our static transform publisher to publish an incorrect transform. We found the issue after several days of being perplexed: Baxter had recently restarted and upon startup the camera resolution was 320x200 while our code assumed that the Baxter wrist camera was at the full resolution of 1280x800. As soon as we fixed this, the transform became accurate and our static transform publisher correctly published the transform.

Difficulties in Object Detection and Classification:
The first major difficulty in object detection was figuring out an effective way to separate our kitchenware of interest from the background. We tried out several different of the methods used in Lab 6 (RGB Color Thresholding, Edge Detection, K-Means Clustering), but we found that none of them met our requirements of reliably separating the object while doing so in a computationally feasible time. After looking into other options online and confirming with Fayyaz's previous image analysis experience, we decided on using the hue-saturation-value (HSV) color space with the assumption that all of our objects are colored white, because most typical kitchenware is white. We also originally had a color threshold for brightly colored objects (ie. neon green, light red, sky blue, etc), but we found that detecting these objects was highly dependent upon the lighting condition even in the HSV color space. White objects under the HSV color space typically cluster in the same general region despite lighting conditions, so our color thresholding worked quite reliably on them at the several different locations in the room we tested it.

To classify the object in the camera frame as a plate, cup, or utensil, we utilized Principal Component Analysis (PCA) on the segmented pointcloud. To compare different sized objects, we normalized all the PCA components by the first PCA component, and when we did, we found that the plate consistently had its PC1 and PC2 both around 1 with a PC3 of about 0, and the fork/spoon had the PC2 consistently below 0.5 or so with a variable PC3. However, we couldn't find any sort of recognizable pattern in the cup's PCA of its pointcloud. This was because so much of the detected cup depended on the angle of the Realsense towards the cup, how much of the inside of the cup was visible, and how much of the base of the cup was obscured by shadow. Consequently, we couldn't write an effective conditional statement to classify an object as a cup, so we made the assumption that the only type of objects in the field of view could be plates, cups, or spoons, and left the cup condition as an else statement if the plate and cup PC thresholds failed. Ideally in the future, we would like to devise a setup that allows for a constant camera angle and lighting environment so that we could develop a more robust classification algorithm without that limiting assumption.

Difficulties in Grasping Kitchenware:
Cup and Fork: One of the major difficulties in grasping the cup was handling how thin the rim was and making sure that our algorithm was accurate enough that errors because of the pointcloud were corrected for. When utilizing only the position information from the Realsense camera, we found that by using the median of the pointcloud, we could be telling the robot that the center of the cup was one or two centimeters away from where it actually was. Since the error needed to be even smaller than that to consistently grasp the cup, we instructed Baxter to move a few centimeters in front of the cup, where it takes a picture of the cup from a pre-determined height. We utilized the same color thresholding on an image from the Baxter wrist camera to detect where the cup was, assigned that as the new center of the cup, which corrected any errors, and allowed Baxter to pick up the cup very consistently. We used the same procedure for the fork to correct for similar errors because the precision needed to be about the same as the rim of the cup to pick up the plastic fork at its neck. Another smaller difficulty we faced was getting our system to work on several different tables, since we simplified our testing to a particular table height, but this would be a quick fix if we used the height of the AR-tag when doing the calibration between the Baxter and Realsense.

Plate: Since the most stable way to grasp the plate is similar to how a human hand grasps it, approaching from the side and grasping the top and bottom of the plate, we programmed the robot to follow that motion. Baxter would lower its forearm to a horizontal position, position the tip of the gripper over the center of the plate, move its forearm back towards its body by the radius of the plate, lower its forearm to table height, and slide its forearm forward to pick up the plate. The major difficulty came in the motion planning for getting Baxter's forearm to a horizontal position. If the plate was too close to Baxter's body, even if technically in a reachable position by Baxter, the motion planning for the plate often failed because Baxter couldn't get its arm into the position with its forearm horizontal. Sometimes even if it did, it couldn't retract its arm away from the plate by a distance equal to the radius of the plate. This is a major difficulty that we were still working on fixing in the last stages of our project while improving the reliability of the plate pickup.

## Potential Improvements

We used AR-tags to distinguish the different objects but ideally we would want to program the robot to be able to identify different objects without AR-tags or be able to have it see and recognize smaller tags that we could attach to the objects.

We would also want to find a way to make the AR-tag tracker update and find the tf transformations faster. At the moment, our robot pauses every few seconds in order to find the tag and recalculate the transforms and see how far and at what angle the tag is after the robot has moved so that it will stay on track to get to the target.

Additionally, our robot's arm is hardcoded at the moment to always reach to the same point in front of the robot. Given more time, we would have attempted to implement a program to perform inverse kinematics on the location of the object and the arm's position and we would have also implemented controls on the arm as well so that it would reach towards the object instead of moving to the same spot. 

Finally, we would have liked to make a URDF (Universal Robot Description Format) of our robot in order to use RViz MoveIt Control.
