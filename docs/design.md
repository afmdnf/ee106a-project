---
order: 10
---

# Design

## Design Criteria

Our group's primary design criterion was to be able to reliably pick up 3 different object types — plates, cups and forks — and move them to a desired location, effectively sorting all objects in the workspace. This naturally required enforcement of a secondary criterion: namely, the ability to reliably detect both the number of objects in the workspace, as well each object's type and position, with high confidence. Finally, we wanted our system to be able to perform this detection continuously, and update Baxter with the state of the workspace in real time.

## Our Design

The final design of our system uses images from a RealSense's standard camera to isolate all objects of interest from a colorless background. Binary image masks for each object are then used to segment a pointcloud obtained from the RealSense's depth camera. Each object's pointcloud is analyzed using 3-dimensional principal component analysis (PCA) to determine its type and orientation. This information is then continuously published to a ROS topic, where an action-planning node using the MoveIt interface can instruct Baxter to move a parallel-jaw gripper to the object's location, pick it up, and set it in a designated location. During pickup, rudimentary visual servoing using image data from the Baxter wrist camera is used to fine-tune the position of the gripper.

## Design Choices and Tradeoffs

* When making our design, we chose to use the RealSense camera and the Baxter model, since these materials were readily available in lab and we were initially planning to perform two-handed manipulation, which would not be possible with the Sawyers provided in lab. Using an external depth camera made it much easier to provide an overview of the whole workspace at all times than using purely the Baxter cameras/depth sensors. However, this required us to spend a significant amount of time developing a calibration system for localizing the RealSense with respect to the Baxter, using a common AR marker.
* To keep our robot simple, we chose to only classify three different object types: cups, plates and forks. These objects were chosen because they are very distinct in their point distributions — cups are primarily distributed in 3 dimensions, plates in 2, and forks/other utensils in only 1 — making it easier to classify each object quickly, using simple thresholds on normalized singular values. However, this severely restricted the possible objects we could use to test the system, meaning the techniques we used would not scale well with additional object types, such as bowls.
* Due to time constraints, moving the Baxter end-effector to each object's position and picking the object up was done primarily with open-loop control and a series of hard-coded movements. To adjust for errors in the initial location of the gripper, we used only a single image from from the Baxter wrist camera to make small corrections. While this solution worked fairly well, it certainly isn't optimal for making the system more flexible and usable in a variety of different conditions, where closed-loop control using real-time data from the wrist cameras and Baxter's built-in depth sensors would be much more effective.
* We wanted our device to work while making no assumptions about each object's color, beyond the fact that it must be a different color than the background table. For example, we couldn't assume that all plates were red, which would allow the use of simple RGB thresholding to identify each object type. While this allows our system to adapt more easily to objects with a wide range of colors and shapes, it required us to use more complicated computer vision techniques to isolate and identify each object.

## Design Choices' Impact

* Using a RealSense with the Baxter improves the efficiency of system operation, since it can constantly survey the entire workspace, whereas using only the Baxter cameras would introduce issues involving camera positioning, limitations on the field-of-view of the workspace, and the need to identify pixels in separate images as belonging to the same object. However, at the same time, the need to mount a camera on a tripod, and calibrate its location with Baxter using an AR tag would decrease both time and space efficiency. One solution would be to mount the camera in an out-of-the-way position directly to the Baxter, so that its location relative to the Baxter can be hard-coded, without need for calibration.
* Classiying all objects using basic variance thresholding, while fast, is not conducive to the robustness of the system, since it means that new techniques must be introduced to handle other types of kitchenware (e.g. bowls), as well as other kitchenware of the same object types, but with different shapes and sizes (e.g. different size spoons).
* In the real world, using only open-loop control does not allow much flexibility in handling objects of different shapes and sizes, and is also not very robust, since errors in picking up objects, such as due to a failed grasp or variations in table height, would not be corrected quickly.
* We felt that using only pointcloud shape data to classify objects is vital to the robustness of the system, since in the real world, kitchenware can come in all colors, shapes and sizes, and a purely color-based system would be ineffective in distinguishing a white cup from a white plate.
